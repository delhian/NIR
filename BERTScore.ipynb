{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NIR/blob/main/BERTScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee156ac6",
      "metadata": {
        "id": "ee156ac6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NIR/blob/main/%D0%9D%D0%98%D0%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6UsVeUsvRob1",
      "metadata": {
        "id": "6UsVeUsvRob1"
      },
      "outputs": [],
      "source": [
        "# !pip3 install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1028dc3f",
      "metadata": {
        "id": "1028dc3f"
      },
      "outputs": [],
      "source": [
        "# !pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc03b01",
      "metadata": {
        "scrolled": true,
        "id": "2bc03b01"
      },
      "outputs": [],
      "source": [
        "# pip install word2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed488de",
      "metadata": {
        "id": "bed488de"
      },
      "outputs": [],
      "source": [
        "# pip install qc-procrustes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14955a30",
      "metadata": {
        "scrolled": true,
        "id": "14955a30"
      },
      "outputs": [],
      "source": [
        "# translator.translate('veritas lux mea', src='la')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OR1FAw0FHPuR",
      "metadata": {
        "id": "OR1FAw0FHPuR",
        "outputId": "3a660c32-7d82-4472-af36-8f402ebd5001"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div class=\"bk-root\">\n",
              "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
              "        <span id=\"1003\">Loading BokehJS ...</span>\n",
              "    </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(\"1003\");\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error(url) {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n",
              "\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      if (url in hashes) {\n",
              "        element.crossOrigin = \"anonymous\";\n",
              "        element.integrity = \"sha384-\" + hashes[url];\n",
              "      }\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  \n",
              "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n",
              "  var css_urls = [];\n",
              "  \n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    function(Bokeh) {\n",
              "    \n",
              "    \n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "      \n",
              "    for (var i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "    if (force === true) {\n",
              "        display_loaded();\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1003\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import umap.umap_ as umap\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "from bokeh.models import ColumnDataSource, LabelSet, Legend\n",
        "from bokeh.plotting  import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances, pairwise_distances, cosine_similarity, cosine_distances\n",
        "# https://arxiv.org/pdf/1911.12019v1.pdf\n",
        "\n",
        "from deep_translator import GoogleTranslator\n",
        "gt = GoogleTranslator(source='ru', target='en')\n",
        "\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "from procrustes import orthogonal, generic, rotational\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import random as rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b45999",
      "metadata": {
        "scrolled": false,
        "id": "10b45999"
      },
      "outputs": [],
      "source": [
        "# # pairs= pd.read_excel('pairs.xlsx', names = \\\n",
        "# #                      ['en', 'sarn','sarn_score','kurl', 'kurl_score', 'google', 'google_score', 'sum'])\n",
        "# # pairs = pairs[['en','sarn','kurl', 'google', ]]\n",
        "\n",
        "# pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DoN0t_F66LDa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a4a3f78006494ac383de6817aa8fd3e3",
            "d5a3ef27ea9846bd9654f0b34a3b791e",
            "ba712bff7765437292527854a45895be",
            "b71b32eb2b254fde93f8cc0fbc96adf9",
            "21f56331699e4322b4c6b4ed5d2d8ab4"
          ]
        },
        "id": "DoN0t_F66LDa",
        "outputId": "41f0f73b-62ce-4046-a69e-6c5d2af03897"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4a3f78006494ac383de6817aa8fd3e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5a3ef27ea9846bd9654f0b34a3b791e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba712bff7765437292527854a45895be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b71b32eb2b254fde93f8cc0fbc96adf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21f56331699e4322b4c6b4ed5d2d8ab4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 8min 15s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# model_name ='bert-base-uncased'\n",
        "model_name ='bert-base-multilingual-cased'\n",
        "model_en = AutoModel.from_pretrained(model_name)\n",
        "tokenizer_en = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "model_name ='DeepPavlov/rubert-base-cased'\n",
        "# model_name ='DeepPavlov/rubert-base-cased-sentence'\n",
        "tokenizer_ru = AutoTokenizer.from_pretrained(model_name)\n",
        "model_ru = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefd29e9",
      "metadata": {
        "id": "aefd29e9",
        "outputId": "d78df597-111f-4d70-f056-d7134e2b01ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1382371\n",
            "1398883\n",
            "1423133\n",
            "1364074\n",
            "1350316\n",
            "Wall time: 132 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "name = './Stephen King - Needful Things.txt'\n",
        "with open(name, 'r',) as file:\n",
        "    text_en = file.read()\n",
        "print(len(text_en))\n",
        "\n",
        "name_ru = './Нужные вещи_Сарнов.txt'\n",
        "with open(name_ru, 'r', encoding='utf-8') as file:\n",
        "    text_ru_sarn = file.read()\n",
        "print(len(text_ru_sarn))\n",
        "\n",
        "name_ru = './Нужные вещи_Курляндская.txt'\n",
        "with open(name_ru, 'r', encoding='windows-1251') as file:\n",
        "    text_ru_kurl = file.read()\n",
        "print(len(text_ru_kurl))\n",
        "\n",
        "name_ru = './Нужные-вещи_Аракелов.txt'\n",
        "with open(name_ru, 'r', encoding='utf-16') as file:\n",
        "    text_ru_arak = file.read()\n",
        "print(len(text_ru_arak))\n",
        "\n",
        "name_ru = './Нужные вещи_Гугл.txt'\n",
        "with open(name_ru, 'r', encoding='utf-8') as file:\n",
        "    text_ru_google = file.read()\n",
        "print(len(text_ru_google))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lz-V_zmke1Fx",
      "metadata": {
        "id": "lz-V_zmke1Fx"
      },
      "outputs": [],
      "source": [
        "# pairs = []\n",
        "# for ends in ['ый', 'ая', 'ое', 'ые']:\n",
        "#   pairs.append(pd.read_csv('/content/drive/MyDrive/pairs_' + ends+'.csv'))\n",
        "# pairs = pd.concat(pairs)\n",
        "# pairs = pairs.reset_index(drop=True).drop_duplicates()\n",
        "# pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h_kubft-ZbMo",
      "metadata": {
        "id": "h_kubft-ZbMo"
      },
      "outputs": [],
      "source": [
        "# phrases_ru = pd.DataFrame([['бел', 'white'], \n",
        "#                            ['черн', 'black'],\n",
        "#                            ['красн', 'red'],\n",
        "#                            ['желт', 'yellow'],\n",
        "#                            ['зелен', 'green'],\n",
        "#                            ['голуб', 'blue'],\n",
        "#                            ['син', 'blue'],\n",
        "#                            ['коричнев', 'brown'],\n",
        "#                            ['фиолетов','violet'],\n",
        "#                            ['оранжев', 'orange'],\n",
        "#                            ['розов', 'pink'],\n",
        "#                            ['сер', 'gray'],['сер', 'grey']], columns = ['base', 'token_en'])\n",
        "\n",
        "\n",
        "# ru_ends = ['ая', 'ее', 'ого', 'ое', 'ой','ом','ому','ою','ую','ые','ый','ым','ыми','ых']\n",
        "              \n",
        "# phrases_ru = pd.DataFrame([[x, x + y] for y in sorted(ru_ends) for x in phrases_ru['base'].values],\\\n",
        "# columns = ['base','token']).set_index('base').join(phrases_ru.set_index('base')).reset_index()\n",
        "\n",
        "# phrases_en = pd.DataFrame([['white'], \n",
        "#                            ['black'],\n",
        "#                            ['red'],\n",
        "#                            ['yellow'],\n",
        "#                            ['green'],\n",
        "#                            ['blue'],\n",
        "#                            ['brown'],\n",
        "#                            ['violet'],\n",
        "#                            ['orange'],\n",
        "#                            ['pink'],\n",
        "#                            ['gray'],\n",
        "#                            ['grey']], columns = {'base'})\n",
        "\n",
        "# phrases_en['token'] = phrases_en['base'] \n",
        "# phrases_en['en'] = phrases_en['base']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gv9PRUOmjs5v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "015b98f0e22d41a19a174e4062f16925",
            "5876cec1c0434bd4b9824a4c55214b1b",
            "6e1f2bd880b84f4bada98775b262c924",
            "957f6413d8174c6f9d81f1683bf3601d",
            "293e0fc2485a4dd89ba76fd79a523bb8"
          ]
        },
        "id": "Gv9PRUOmjs5v",
        "outputId": "964d0d0a-a1db-4de1-9d36-cea63a78c5a1",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "015b98f0e22d41a19a174e4062f16925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/19178 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(ru_to_en_dict): 0\n",
            "len(ru_to_en_dict): 4000\n",
            "len(ru_to_en_dict): 9000\n",
            "len(ru_to_en_dict): 10000\n",
            "len(ru_to_en_dict): 14000\n",
            "len(ru_to_en_dict): 18000\n",
            "len(ru_to_en_dict): 19000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5876cec1c0434bd4b9824a4c55214b1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/19823 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(ru_to_en_dict): 23000\n",
            "len(ru_to_en_dict): 23000\n",
            "len(ru_to_en_dict): 23000\n",
            "len(ru_to_en_dict): 24000\n",
            "len(ru_to_en_dict): 24000\n",
            "len(ru_to_en_dict): 24000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 25000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 26000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n",
            "len(ru_to_en_dict): 28000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e1f2bd880b84f4bada98775b262c924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20014 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(ru_to_en_dict): 29000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 30000\n",
            "len(ru_to_en_dict): 31000\n",
            "len(ru_to_en_dict): 31000\n",
            "len(ru_to_en_dict): 31000\n",
            "len(ru_to_en_dict): 31000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n",
            "len(ru_to_en_dict): 32000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "957f6413d8174c6f9d81f1683bf3601d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20646 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n",
            "len(ru_to_en_dict): 34000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "293e0fc2485a4dd89ba76fd79a523bb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/21004 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(296988, 5) (321719, 5) (316871, 5) (301547, 5) (325032, 5)\n",
            "Wall time: 4h 23min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "# def get_empty(x):\n",
        "#     try:\n",
        "#         tmp = re.findall('^[a-z]+$', x)\n",
        "#         return tmp[0]\n",
        "#     except:\n",
        "#         return ''\n",
        "\n",
        "ru_sentences_re = '[А-Я].*?[\\.!?]'\n",
        "en_sentences_re = '[A-Z].*?[\\.!?]'\n",
        "\n",
        "ru_to_en_dict ={}\n",
        "\n",
        "def f_dfmake(text,\n",
        "             sentences_re,\n",
        "             tokenizer,\n",
        "             model,\n",
        "             translate):\n",
        "    emb =[]\n",
        "    prog = re.compile(sentences_re, re.MULTILINE | re.DOTALL )\n",
        "    sentences = re.findall(prog, text)\n",
        "    for txt in tqdm(sentences):\n",
        "        ru_to_en_dict_len = len(ru_to_en_dict)\n",
        "        if ru_to_en_dict_len%1000 == 0:\n",
        "            print(f'len(ru_to_en_dict): {ru_to_en_dict_len}')\n",
        "        emb_token = []\n",
        "#         txt = text[seq.start() + 1: seq.end()]\n",
        "        encoded_input = tokenizer(txt, return_tensors='pt')\n",
        "        sentence_embedding = model(**encoded_input)[0][0,1:-1]\n",
        "        for ind, token in enumerate(tokenizer.tokenize(txt)):\n",
        "            word_embedding = sentence_embedding[ind].detach().numpy()\n",
        "# --normilize:\n",
        "            word_embedding /= np.sqrt(np.sum(word_embedding ** 2))\n",
        "            token_lower = token.lower()\n",
        "            if translate:\n",
        "                token_en = ru_to_en_dict.get(token_lower, '')\n",
        "                if token_en =='':\n",
        "                    try:\n",
        "                        token_en = gt.translate(token_lower).lower()\n",
        "                    except:\n",
        "                        token_en = token\n",
        "                    ru_to_en_dict[token_lower] = token_en\n",
        "            else:\n",
        "                token_en = token_lower\n",
        "            emb_token.append([token, token_lower, token_en, txt , word_embedding, ])  \n",
        "        emb += emb_token\n",
        "    emb = pd.DataFrame(emb, columns = ['token', 'token_lower', 'token_en','txt', 'word_embedding'])\n",
        "    return emb\n",
        "\n",
        "emb_ru_google = f_dfmake (text_ru_google, ru_sentences_re, tokenizer_ru, model_ru, True)\n",
        "emb_ru_sarn = f_dfmake (text_ru_sarn, ru_sentences_re, tokenizer_ru, model_ru, True)\n",
        "emb_ru_kurl = f_dfmake (text_ru_kurl, ru_sentences_re, tokenizer_ru, model_ru, True)\n",
        "emb_ru_arak = f_dfmake (text_ru_arak, ru_sentences_re, tokenizer_ru, model_ru, True)\n",
        "emb_en = f_dfmake (text_en, en_sentences_re, tokenizer_en, model_en, False)\n",
        "\n",
        "print(emb_ru_google.shape, emb_ru_sarn.shape,emb_ru_kurl.shape, emb_ru_arak.shape, emb_en.shape)\n",
        "emb_ru_google.to_pickle('emb_ru_google.pkl')\n",
        "emb_ru_sarn.to_pickle('emb_ru_sarn.pkl')\n",
        "emb_ru_kurl.to_pickle('emb_ru_kurl.pkl')\n",
        "emb_ru_arak.to_pickle('emb_ru_arak.pkl')\n",
        "emb_en.to_pickle('emb_en.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1657ad9",
      "metadata": {
        "id": "f1657ad9",
        "outputId": "5872e2c7-770a-4a83-e7a8-e4a8bd99e08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(296988, 5) (321719, 5) (316871, 5) (325032, 5)\n",
            "Wall time: 13.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "emb_ru_google = pd.read_pickle('emb_ru_google_rubert-base-cased.pkl')\n",
        "emb_ru_sarn = pd.read_pickle('emb_ru_sarn_rubert-base-cased.pkl')\n",
        "emb_ru_kurl = pd.read_pickle('emb_ru_kurl_rubert-base-cased.pkl')\n",
        "# emb_ru_arak = pd.read_pickle('emb_ru_arak_rubert-base-cased.pkl')\n",
        "emb_en = pd.read_pickle('emb_en.pkl')\n",
        "\n",
        "emb_ru_list = [emb_ru_sarn, emb_ru_kurl, emb_ru_google]\n",
        "\n",
        "print(emb_ru_google.shape, emb_ru_sarn.shape,emb_ru_kurl.shape,  emb_en.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42d1165d",
      "metadata": {
        "id": "42d1165d"
      },
      "source": [
        "## Нам нужно получить \"анкерные\" слова -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb07cc2",
      "metadata": {
        "id": "9cb07cc2"
      },
      "outputs": [],
      "source": [
        "class AbsoluteOrientation:\n",
        "    def __init__(self, R=None, Y_mean=None):\n",
        "        self.R = R\n",
        "        self.Y_mean = Y_mean\n",
        "    def fit(self, X, Y):\n",
        "        X = np.stack(X)\n",
        "        Y = np.stack(Y)\n",
        "        # orthogonal, generic, rotational\n",
        "        self.R = orthogonal(X, Y, ).t\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        X = np.stack(X)\n",
        "        return list(np.dot(X, self.R))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e651018",
      "metadata": {
        "scrolled": true,
        "id": "7e651018"
      },
      "outputs": [],
      "source": [
        "def f_train(emb_ru, emb_en):\n",
        "\n",
        "    def get_empty_en(x):\n",
        "    # --берем только слова с >1 буквами и цифрами\n",
        "        try:\n",
        "            tmp = re.findall('^[a-z0-9]{1,}$', x)\n",
        "            return tmp[0]\n",
        "        except:\n",
        "            return ''\n",
        "        \n",
        "    def get_empty_ru(x):\n",
        "    # --берем только слова с >1 буквами и цифрами\n",
        "        try:\n",
        "            tmp = re.findall('^[а-я0-9]{1,}$', x)\n",
        "            return tmp[0]\n",
        "        except:\n",
        "            return ''\n",
        "             \n",
        "#     убираем неполные слова :   \n",
        "    emb_ru = emb_ru[(emb_ru['token_lower'].shift(-1).str[0] != '#')&(emb_ru['token_lower'].str[0] != '#')]\n",
        "#     убираем редкие слова : \n",
        "    most_frequent_tokens = emb_ru.groupby('token_en')['token_lower'].agg(lambda x:x.value_counts().index[0]).values\n",
        "    emb_ru = emb_ru[emb_ru['token_lower'].isin(most_frequent_tokens )] \n",
        "#     убираем короткие слова  \n",
        "    emb_ru = emb_ru[emb_ru['token_lower'].apply(get_empty_ru) != '']    \n",
        "    emb_ru = emb_ru[emb_ru['token_en'].apply(get_empty_en) != '']\n",
        "\n",
        "    def get_one_vector(x):\n",
        "        tmp = np.array(x['word_embedding'].values.tolist())\n",
        "        d = {}\n",
        "        d['word_embedding'] = np.mean(tmp, axis=0)\n",
        "        d['count'] = tmp.shape[0]\n",
        "        return pd.Series(d, index=['word_embedding', 'count'])\n",
        "\n",
        "    # too_rare  -- убираем слишком редкие слова\n",
        "    too_rare = 10\n",
        "\n",
        "    tmp_ru = emb_ru.groupby(['token_en'], as_index = True).apply(get_one_vector).reset_index()\n",
        "    tmp_ru = tmp_ru[tmp_ru['count'] > too_rare]\n",
        "    tmp_ru = tmp_ru[['token_en','word_embedding']]\n",
        "\n",
        "    tmp_en = emb_en.groupby(['token_en'], as_index = True).apply(get_one_vector).reset_index()\n",
        "    tmp_en = tmp_en[tmp_en['count'] > too_rare]\n",
        "    tmp_en = tmp_en[['token_en','word_embedding']]\n",
        "\n",
        "    embeddings_matched = tmp_ru.set_index('token_en').join(tmp_en.set_index('token_en'), \\\n",
        "                                      how = 'inner', lsuffix = '_ru', rsuffix = '_en')\n",
        "    print(f'nof unique words matched: {embeddings_matched.shape[0]}')\n",
        "\n",
        "    pcd = paired_cosine_distances(embeddings_matched.iloc[:, 0].values.tolist(), \\\n",
        "                                  embeddings_matched.iloc[:, 1].values.tolist())\n",
        "\n",
        "    print(f'pcd-before alignment: {np.mean(pcd)}', end = ', ')\n",
        "\n",
        "    ao = AbsoluteOrientation()\n",
        "\n",
        "    ao.fit(embeddings_matched.iloc[:, 0], embeddings_matched.iloc[:, 1])\n",
        "\n",
        "    embeddings_matched['word_embedding_ru'] = ao.predict(embeddings_matched.iloc[:, 0])\n",
        "    \n",
        "    pcd = paired_cosine_distances(embeddings_matched.iloc[:, 0].values.tolist(), embeddings_matched.iloc[:, 1].\\\n",
        "                                  values.tolist())\n",
        "    print(f'pcd-after alignment: {np.mean(pcd)}')\n",
        "    \n",
        "    ix_max = np.argmax(pcd)\n",
        "    ix_min = np.argmin(pcd)\n",
        "    print(f'smallest cd = {pcd[ix_min]}, en_token = {embeddings_matched.index[ix_min]}', end = ', ')\n",
        "    print(f'biggest cd = {pcd[ix_max]}, en_token = {embeddings_matched.index[ix_max]}')\n",
        "    return ao, embeddings_matched"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb423763",
      "metadata": {
        "id": "eb423763"
      },
      "source": [
        "# Преобразовываем все слова:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f2161b",
      "metadata": {
        "id": "e5f2161b",
        "outputId": "d8bc9d33-f26b-4a77-85e4-48f8d14c085e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nof unique words matched: 1251\n",
            "pcd-before alignment: 0.9967246055603027, pcd-after alignment: 0.27649641036987305\n",
            "smallest cd = 0.14409250020980835, en_token = he, biggest cd = 0.5158604383468628, en_token = besides\n",
            "Wall time: 19.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "ao, embeddings_matched = f_train(pd.concat([emb_ru_sarn, emb_ru_kurl, emb_ru_google, ]), emb_en)\n",
        "\n",
        "for i in range(len(emb_ru_list)):\n",
        "    emb_ru_list[i]['word_embedding'] = ao.predict(emb_ru_list[i]['word_embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0732369",
      "metadata": {
        "scrolled": true,
        "id": "a0732369",
        "outputId": "435d7f95-7058-4756-8164-fa0496f0235d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6193    зеленый\n",
            "Name: token_lower, dtype: object\n",
            "0.5860814\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>token_lower</th>\n",
              "      <th>token_en</th>\n",
              "      <th>txt</th>\n",
              "      <th>word_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6801</th>\n",
              "      <td>green</td>\n",
              "      <td>green</td>\n",
              "      <td>green</td>\n",
              "      <td>She was\\nalready dialling Myra, and they were soon discussing the green awning\\nwith great enthusiasm.</td>\n",
              "      <td>[-0.012329235, 0.026654841, 0.020789495, 0.017201733, -0.04070474, 0.032074496, -0.021143457, 0.042171177, -0.032908995, 0.018147502, -0.008657807, -0.001499812, -0.017121714, -0.015354901, 0.018883329, -0.018751258, -0.014211274, -0.0070660403, -0.0007140327, -0.004839001, -0.0208506, 0.012041258, -0.008456515, 0.007113686, 0.007918781, -0.004700831, 0.0012893945, 0.0397399, -0.01573223, 0.04...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token token_lower token_en                                                                                                     txt                                                                                                                                                                                                                                                                                                                                                                                                   word_embedding\n",
              "6801  green       green    green  She was\\nalready dialling Myra, and they were soon discussing the green awning\\nwith great enthusiasm.  [-0.012329235, 0.026654841, 0.020789495, 0.017201733, -0.04070474, 0.032074496, -0.021143457, 0.042171177, -0.032908995, 0.018147502, -0.008657807, -0.001499812, -0.017121714, -0.015354901, 0.018883329, -0.018751258, -0.014211274, -0.0070660403, -0.0007140327, -0.004839001, -0.0208506, 0.012041258, -0.008456515, 0.007113686, 0.007918781, -0.004700831, 0.0012893945, 0.0397399, -0.01573223, 0.04..."
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp= emb_ru_sarn\n",
        "tmp = tmp[tmp.token_lower=='зеленый'].sample(1)\n",
        "print(tmp.token_lower)\n",
        "\n",
        "d = cosine_similarity(tmp.word_embedding.values[0].reshape(1, -1), emb_en.word_embedding.values.tolist())[0]\n",
        "ix_mx = np.argmax(d)\n",
        " \n",
        "print(d[ix_mx])\n",
        "emb_en.iloc[[ix_mx]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fef1cc6",
      "metadata": {
        "id": "1fef1cc6"
      },
      "source": [
        "## Считаем BERTScore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2dd2791",
      "metadata": {
        "id": "a2dd2791",
        "outputId": "a5c85e26-729e-450a-f1f1-1f504fa18f53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(58, 8)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs= pd.read_excel('resorch.xlsx', names = \\\n",
        "                     ['en', 'sarn','sarn_score','kurl', 'kurl_score', 'google', 'google_score', 'sum'])\n",
        "pairs = pairs.dropna()\n",
        "\n",
        "pairs['sarn_score']  = pairs['sarn_score'].astype('int')\n",
        "pairs['kurl_score']  = pairs['kurl_score'].astype('int')\n",
        "pairs['google_score']  = pairs['google_score'].astype('int')\n",
        "pairs['sum']  = pairs['sum'].astype('int')\n",
        "\n",
        "pairs = pairs[(pairs['sum'] == 3)& (pairs['sarn_score'] * pairs['kurl_score'] * pairs['google_score'] == 0)]\n",
        "\n",
        "# pairs = pairs.head(1)\n",
        "\n",
        "pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b5bc7a",
      "metadata": {
        "scrolled": true,
        "id": "43b5bc7a",
        "outputId": "2f10309e-f008-4af7-aed6-8887ac91d33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 9.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "for ind, candidate in enumerate(['sarn', 'kurl', 'google']):\n",
        "#     print('*'*100)\n",
        "#     print(candidate)\n",
        "    emb_ru = emb_ru_list[ind]\n",
        "    for ix, row in pairs[['en', candidate]].iterrows():\n",
        "#         print(row.en, '\\n', row[candidate], )\n",
        "        tmp_en = emb_en[emb_en.txt == row.en]\n",
        "        tmp_ru = emb_ru[emb_ru.txt == row[candidate]]\n",
        "        precision = []\n",
        "        for _, row_ru in tmp_ru.iterrows():\n",
        "#             print(row_ru.token_lower, end = ':')\n",
        "            d = cosine_similarity(row_ru.word_embedding.reshape(1, -1), \\\n",
        "                                      tmp_en.word_embedding.values.tolist())[0]\n",
        "#             print(d)\n",
        "            ix_mx = np.argmax(d)\n",
        "#             print(tmp_en.iloc[ix_mx].token_en,': ', d[ix_mx])\n",
        "            precision.append(d[ix_mx])\n",
        "\n",
        "        precision = np.mean(precision)\n",
        "\n",
        "        recall = []\n",
        "        for _, row_en in tmp_en.iterrows():\n",
        "#             print(row_en.token_lower, end = ':')\n",
        "            d = cosine_similarity(row_en.word_embedding.reshape(1, -1), \\\n",
        "                                      tmp_ru.word_embedding.values.tolist())[0]\n",
        "#             print(d)\n",
        "            ix_mx = np.argmax(d)\n",
        "#             print(tmp_ru.iloc[ix_mx].token_lower,': ', d[ix_mx])\n",
        "            recall.append(d[ix_mx])\n",
        "        recall = np.mean(recall)\n",
        "\n",
        "#         print(precision, recall)\n",
        "        \n",
        "        pairs.loc[ix, candidate +'_p'] = precision\n",
        "        pairs.loc[ix, candidate +'_r'] = recall\n",
        "        pairs.loc[ix, candidate +'_f'] = 2* precision*recall/(precision+recall)\n",
        "        \n",
        "pairs =pairs[['en', 'sarn', 'kurl', 'google', \\\n",
        "              'sarn_score', 'kurl_score', 'google_score', 'sarn_f', 'kurl_f', 'google_f']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4915e0f",
      "metadata": {
        "id": "a4915e0f"
      },
      "outputs": [],
      "source": [
        "def f_get_spearmanr(x):\n",
        "    t1 = x[['sarn_score', 'kurl_score', 'google_score']]\n",
        "    t2 = x[['sarn_f', 'kurl_f', 'google_f']]\n",
        "    return stats.spearmanr(t1, t2).correlation\n",
        "\n",
        "pairs['spearmanr'] = pairs.apply(f_get_spearmanr, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a448a5",
      "metadata": {
        "id": "e3a448a5",
        "outputId": "10afc716-51e0-40d8-80f9-83826298358e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.3620689655172414"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs['spearmanr'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e6560e",
      "metadata": {
        "scrolled": true,
        "id": "10e6560e",
        "outputId": "4b126826-f487-4695-8dae-e65177d2653a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>sarn</th>\n",
              "      <th>kurl</th>\n",
              "      <th>google</th>\n",
              "      <th>sarn_score</th>\n",
              "      <th>kurl_score</th>\n",
              "      <th>google_score</th>\n",
              "      <th>sarn_f</th>\n",
              "      <th>kurl_f</th>\n",
              "      <th>google_f</th>\n",
              "      <th>spearmanr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>He produced a plain white card from\\nhis back pocket, opened the mail-slot, and dropped the card through.</td>\n",
              "      <td>Из заднего кармана он достал белую карточку, открыл щель для почты, просунул туда карточку и кончиками пальцев придержал крышку прорези, чтобы она не щелкнула, когда закрывалась.</td>\n",
              "      <td>Достав из заднего кармана брюк белую карточку, он сунул ее в почтовую щель на двери.</td>\n",
              "      <td>Он достал простую белую карточку из заднего кармана, открыл прорезь для почты и вытащил карту внутрь.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.461813</td>\n",
              "      <td>0.477418</td>\n",
              "      <td>0.513289</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>While Alan was thinking, she had taken off the tie\\nand dextrously unbuttoned the top two buttons of the white blouse\\nunderneath.</td>\n",
              "      <td>Пока Алан был погружен в свои мысли, она успела снять галстук и ловко расстегнуть две верхние пуговки белой блузки под джемпером.</td>\n",
              "      <td>Теперь же платочек был развязан и Полли расстегивала две верхние пуговки белой блузки, надетой под джемпер.</td>\n",
              "      <td>Пока Алан думал, она сняла галстук и ловко расстегнула две верхние пуговицы белой блузки под ним .</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.457043</td>\n",
              "      <td>0.422092</td>\n",
              "      <td>0.480357</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>He was\\nwearing a blue tee-shirt-what the schoolkids called a muscle-shirt-and\\nblue sweatpants with a white stripe on the side.</td>\n",
              "      <td>На нем была голубая майка — ребятишки называют такие футболками, — и голубые спортивные штаны с белой полоской по бокам.</td>\n",
              "      <td>На нем были голубая футболка из тех, что мальчики называют борцовками, и синие спортивные брюки с белыми полосками по бокам.</td>\n",
              "      <td>На нем была синяя футболка - то, что школьники называли мускулистой рубашкой - и синие спортивные штаны с белой полосой по бокам.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.449948</td>\n",
              "      <td>0.457202</td>\n",
              "      <td>0.454874</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Pratt was a beefy young Baptist who usually wore blue\\ntee-shirts and blue sweat-pants with a white stripe running down the\\noutside of each leg.</td>\n",
              "      <td>Пратт был жирным молодым баптистом и обычно носил голубые майки и голубые тренировочные штаны (мокрые от пота) с белыми полосками по внешним швам.</td>\n",
              "      <td>Пратт был крепкий телом молодой баптист, всегда ходил в голубых футболках и тренировочных штанах с белыми полосами вдоль обеих штанин снаружи.</td>\n",
              "      <td>Пратт был крепким молодым баптистом, который обычно носил синие футболки и синие спортивные штаны с белой полосой, идущей по внешней стороне каждой ноги.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.471452</td>\n",
              "      <td>0.447446</td>\n",
              "      <td>0.489664</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>She was\\nalready dialling Myra, and they were soon discussing the green awning\\nwith great enthusiasm.</td>\n",
              "      <td>Она уже набирала номер Майры, и сейчас они примутся с большим энтузиазмом обсуждать зеленый навес.</td>\n",
              "      <td>И минуту спустя они с превеликим энтузиазмом обсуждали зеленый навес.</td>\n",
              "      <td>Она уже набирала номер Майры, и вскоре они с большим энтузиазмом обсуждали зеленый навес.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.431667</td>\n",
              "      <td>0.441571</td>\n",
              "      <td>0.466551</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                   en                                                                                                                                                                                sarn                                                                                                                                            kurl                                                                                                                                                     google  sarn_score  kurl_score  google_score    sarn_f    kurl_f  google_f  spearmanr\n",
              "3                                           He produced a plain white card from\\nhis back pocket, opened the mail-slot, and dropped the card through.  Из заднего кармана он достал белую карточку, открыл щель для почты, просунул туда карточку и кончиками пальцев придержал крышку прорези, чтобы она не щелкнула, когда закрывалась.                                                            Достав из заднего кармана брюк белую карточку, он сунул ее в почтовую щель на двери.                                                      Он достал простую белую карточку из заднего кармана, открыл прорезь для почты и вытащил карту внутрь.           1           2             0  0.461813  0.477418  0.513289       -0.5\n",
              "5                  While Alan was thinking, she had taken off the tie\\nand dextrously unbuttoned the top two buttons of the white blouse\\nunderneath.                                                   Пока Алан был погружен в свои мысли, она успела снять галстук и ловко расстегнуть две верхние пуговки белой блузки под джемпером.                                     Теперь же платочек был развязан и Полли расстегивала две верхние пуговки белой блузки, надетой под джемпер.                                                         Пока Алан думал, она сняла галстук и ловко расстегнула две верхние пуговицы белой блузки под ним .           2           1             0  0.457043  0.422092  0.480357       -0.5\n",
              "7                    He was\\nwearing a blue tee-shirt-what the schoolkids called a muscle-shirt-and\\nblue sweatpants with a white stripe on the side.                                                            На нем была голубая майка — ребятишки называют такие футболками, — и голубые спортивные штаны с белой полоской по бокам.                    На нем были голубая футболка из тех, что мальчики называют борцовками, и синие спортивные брюки с белыми полосками по бокам.                          На нем была синяя футболка - то, что школьники называли мускулистой рубашкой - и синие спортивные штаны с белой полосой по бокам.           1           2             0  0.449948  0.457202  0.454874        0.5\n",
              "9   Pratt was a beefy young Baptist who usually wore blue\\ntee-shirts and blue sweat-pants with a white stripe running down the\\noutside of each leg.                                  Пратт был жирным молодым баптистом и обычно носил голубые майки и голубые тренировочные штаны (мокрые от пота) с белыми полосками по внешним швам.  Пратт был крепкий телом молодой баптист, всегда ходил в голубых футболках и тренировочных штанах с белыми полосами вдоль обеих штанин снаружи.  Пратт был крепким молодым баптистом, который обычно носил синие футболки и синие спортивные штаны с белой полосой, идущей по внешней стороне каждой ноги.           0           1             2  0.471452  0.447446  0.489664        0.5\n",
              "17                                             She was\\nalready dialling Myra, and they were soon discussing the green awning\\nwith great enthusiasm.                                                                                  Она уже набирала номер Майры, и сейчас они примутся с большим энтузиазмом обсуждать зеленый навес.                                                                           И минуту спустя они с превеликим энтузиазмом обсуждали зеленый навес.                                                                  Она уже набирала номер Майры, и вскоре они с большим энтузиазмом обсуждали зеленый навес.           1           0             2  0.431667  0.441571  0.466551        0.5"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bc0828",
      "metadata": {
        "id": "c9bc0828"
      },
      "outputs": [],
      "source": [
        "pairs['sarn_f'] = pairs['sarn_f'].map(lambda x:  rn.random())\n",
        "\n",
        "pairs['kurl_f'] = pairs['kurl_f'].map(lambda x:  rn.random())\n",
        "\n",
        "pairs['google_f'] = pairs['google_f'].map(lambda x:  rn.random())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99487e9d",
      "metadata": {
        "id": "99487e9d",
        "outputId": "12301e19-bc5a-497f-c62b-ba0e8dc31997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.77203256"
            ]
          },
          "execution_count": 390,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(tmp_en.reshape(1, -1), tmp_ru.reshape(1, -1) )[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4204d55c",
      "metadata": {
        "id": "4204d55c"
      },
      "source": [
        "## Альтернативный подход: cчитаем cosine_similarity только для цветообозначений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8707d5f5",
      "metadata": {
        "scrolled": true,
        "id": "8707d5f5",
        "outputId": "728b1c79-4e0a-469e-c669-1a8073c2de43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************************************************************************\n",
            "sarn\n",
            "black : pink\n",
            "gray : grey\n",
            "gray : brown\n",
            "****************************************************************************************************\n",
            "kurl\n",
            "black : pink\n",
            "gray : grey\n",
            "gray : grey\n",
            "black : white\n",
            "****************************************************************************************************\n",
            "google\n",
            "white : black\n",
            "gray : green\n",
            "Wall time: 11.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "for ind, candidate in enumerate(['sarn', 'kurl', 'google']):\n",
        "    print('*'*100)\n",
        "    print(candidate)\n",
        "    emb_ru = emb_ru_list[ind]\n",
        "    for ix, row in pairs[['en', candidate]].iterrows():\n",
        "#         print(row.en, '\\n', row[candidate], )\n",
        "\n",
        "        tmp_en = emb_en[(emb_en.txt == row.en)& emb_en.token_en.isin(phrases_en['en'])].head(1)\n",
        "        tmp_ru = emb_ru[(emb_ru.txt == row[candidate])& emb_ru.token_en.isin(phrases_en['en'])].head(1)\n",
        "        \n",
        "        if tmp_en.token_en.values[0] != tmp_ru.token_en.values[0]:\n",
        "            print(tmp_en.token_en.values[0], ':', tmp_ru.token_en.values[0]) \n",
        "    \n",
        "        tmp_en = tmp_en['word_embedding'].values[0]\n",
        "        \n",
        "        tmp_ru = tmp_ru['word_embedding'].values[0]\n",
        "        \n",
        "        \n",
        "        cs = cosine_similarity(tmp_en.reshape(1, -1), tmp_ru.reshape(1, -1) )[0][0]\n",
        "        \n",
        "        pairs.loc[ix, candidate +'_cs'] = cs\n",
        "        \n",
        "pairs = pairs[['en', 'sarn', 'kurl', 'google', \\\n",
        "              'sarn_score', 'kurl_score', 'google_score', 'sarn_cs', 'kurl_cs', 'google_cs']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c721edc8",
      "metadata": {
        "scrolled": true,
        "id": "c721edc8"
      },
      "outputs": [],
      "source": [
        "def f_get_spearmanr(x):\n",
        "    t1 = x[['sarn_score', 'kurl_score', 'google_score']]\n",
        "    t2 = x[['sarn_cs', 'kurl_cs', 'google_cs']]\n",
        "    return stats.spearmanr(t1, t2).correlation\n",
        "\n",
        "pairs['spearmanr'] = pairs.apply(f_get_spearmanr, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca13e6b",
      "metadata": {
        "scrolled": true,
        "id": "fca13e6b",
        "outputId": "4e5b8cff-1e87-4201-aaad-ddd1597cc865"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.07758620689655173"
            ]
          },
          "execution_count": 422,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs['spearmanr'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2809c0e0",
      "metadata": {
        "id": "2809c0e0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BERTScore.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}